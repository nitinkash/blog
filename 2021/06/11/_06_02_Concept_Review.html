<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Backpropagation from scratch | Nitin Kashyap</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Backpropagation from scratch" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="My Data Science Blog" />
<meta property="og:description" content="My Data Science Blog" />
<link rel="canonical" href="https://nitinkash.github.io/blog/2021/06/11/_06_02_Concept_Review.html" />
<meta property="og:url" content="https://nitinkash.github.io/blog/2021/06/11/_06_02_Concept_Review.html" />
<meta property="og:site_name" content="Nitin Kashyap" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-06-11T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"My Data Science Blog","url":"https://nitinkash.github.io/blog/2021/06/11/_06_02_Concept_Review.html","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://nitinkash.github.io/blog/2021/06/11/_06_02_Concept_Review.html"},"headline":"Backpropagation from scratch","dateModified":"2021-06-11T00:00:00-05:00","datePublished":"2021-06-11T00:00:00-05:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://nitinkash.github.io/blog/feed.xml" title="Nitin Kashyap" /><link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" /><script src="https://hypothes.is/embed.js" async></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">Nitin Kashyap</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/">Home</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Backpropagation from scratch</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-06-11T00:00:00-05:00" itemprop="datePublished">
        Jun 11, 2021
      </time>
       â€¢ <span class="read-time" title="Estimated read time">
    
    
      7 min read
    
</span></p>

    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/nitinkash/blog/tree/master/_notebooks/2020_06_02_Concept_Review.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/blog/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/nitinkash/blog/master?filepath=_notebooks%2F2020_06_02_Concept_Review.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/blog/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/nitinkash/blog/blob/master/_notebooks/2020_06_02_Concept_Review.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/blog/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020_06_02_Concept_Review.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="https://colab.research.google.com/github/nitinkash/blog/blob/master/_notebooks/2020_06_02_Concept_Review.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" /></a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>While going through and trying to make a text generator using GANs I realised that my fundamentals in Neural Networks is quite weak an&gt; it had been nearly 4 yoers since I finished my Masters Degree and I had lost touch with a lot of key concepts of Neural NEtworks. While it is possible to do most of these things without knowing all the details. I would personally like to know what I am using so that I avoid potential pitfalls later. I also came across this post while reviewing the videos in Stanford's CS 211n Course. <a href="https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b">link</a>. So will be dopign some porblems from the course assignment and reviewing some key concepts here.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Notation:">Notation:<a class="anchor-link" href="#Notation:"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-four-fundamental-equations-behind-Backpropagation:">The four fundamental equations behind Backpropagation:<a class="anchor-link" href="#The-four-fundamental-equations-behind-Backpropagation:"> </a></h2><p>Backpropgation is essentally a method to understand how changing the weights and biases in the neural network affects the cost function. Mathematically, we want to calculate $\frac{\partial C}{\partial w_{jk}^{l}}$ and $\frac{\partial C}{\partial b_{j}^{l}}$. Before we compute these we introduce an error term $ \delta_{j}^{l}$ which represents the error in the $j^{th}$ neuron of the $l^{th}$ layer.</p>
<h3 id="The-First-Equation">The First Equation<a class="anchor-link" href="#The-First-Equation"> </a></h3><p>The error term is calculated using the equation $$ \delta_{j}^{l} = \frac{\partial C}{\partial a_{j}^{L}} \sigma'(z_{j}^{L})   $$</p>
<p>This equation is a very intuitive result. The $\partial C/ \partial {a_{j}^{L}}$ term measures how fast the Cost function is changing with respect to the activations of the $j^{th}$ output activation. If the Cost Function does not depend on the output of a particular neuron $j$, then the term $ \delta_{j}^{L} $ will be very small, which is as expected. The second term $ \sigma'(z_{j}^{L}) $ measures how fast the activation function $ \sigma$ is changing at $z_{j}^{L}$</p>
<p>Everything in this equation of $\delta _{j}^{L}$ can be computed fairly easily. The exact form of $\partial C/ \partial {a_{j}^{L}}$ will depend on the nature of the cost function used. If we use a quadratic cost function of the form $ C =\frac{1}{2} \Sigma_{j} (y_{j} - a_{j}^{L}) $ then the value of $\partial C/ \partial {a_{j}^{L}} = (a_{j}^{L} - y_{j})$ which can be computed easily.</p>
<p>To avoid the index nightmare we can alse represent this as in matrix notation as $$ \delta^L = \nabla_{a} C \odot \sigma'(z^L) $$. Here, $ \nabla_{a} C$ is basically a vector whose components are $\partial C/ \partial {a_{j}^{L}}$, the rate of change of $C$ with respect to the output activations. For a quadratic loss function $\nabla_a C =(a^L-y)$ so the previous equation reduces to $$ \delta^L = (a^L-y) \odot \sigma'(z^L). $$</p>
<p>Every component in this equation has a vector form that can be calculated easily using Numpy which is the goal of this exercise.</p>
<h3 id="The-Second-Equation:">The Second Equation:<a class="anchor-link" href="#The-Second-Equation:"> </a></h3><p>The second equation of importance is used to calculate the error term $\delta_l$ with the error in the next layer $\delta_{l+1}$</p>
$$
\delta^l = ((w^{l+1})^T \delta^{l+1}) \odot \sigma'(z^l)
$$<p>where $w^{l+1} $ is the weight matrix for the $(l+1)^{th} $ layer. This equation seems complicated, but each element has a nice interpretation. Suppose we know the error $\delta_{l+1}$ at the $l+1^{th}$ layer. When we apply the transpose weight matrix, $(w_{l+1})^T$, we can think intuitively of this as moving the error backward through the network. This gives us a measure of the error at the output of the $l^{th}$ layer. We can then take the Hadamard product $\odot \sigmaâ€²(z^l)$ to calculate the the error $\delta_l$ in the weighted input to layer $l$.</p>
<p>By combining the forst and second equations, we can compute the error $\delta_l$ for any layer in the network. We start by using the first equation to compute $\delta_L$ and then apply the second equation to compute $\delta_{l-1}$, $\delta_{l-2}$, and so on, all the way to the start of the network.</p>
<h3 id="The-Third-equation:">The Third equation:<a class="anchor-link" href="#The-Third-equation:"> </a></h3><p>This equation is related to the rate of cahnge of the Cost function with respect to any bias term in the network. $$ \frac{\partial C}{\partial b^l_j} =
  \delta^l_j $$</p>
<p>This makes the calculation convenient as we already have methods to calculate the  $ \delta_{j}^{l} $ using the first two equations. We can represent this as shorthand by removing the indices after understanding that  $\delta$ is being evaluated at the same neuron as bias $b$.</p>
<h3 id="The-Fourth-Equation:">The Fourth Equation:<a class="anchor-link" href="#The-Fourth-Equation:"> </a></h3><p>This equation fills in the missing piece by providing an expression to calculate the rate of change of the costfunction with respect to any weight in the network.</p>
<p>$$ \frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j $$</p>
<p>This may seem complicated again but when we look at it in terms of a less index heavy representation we can see that it basically states that the rate of change of the Cost function because of the weights is the product of the activation of the neuron input with the error of the neuron output from the weight $w$. $$ \frac{\partial C}{\partial w} = a_{\rm in} \delta_{\rm out} $$</p>
<p>A consequence of this equatioon is that the gradtent term $ \partial C / \partial w $ will be small if the activation $a_{in} \approx 0$. This means that low-activation neurons will learn slowly as it does not change much during Gradient descent.</p>
<p>Another insight from these equations is related to the nature of the activation function. If we use a Sigmoid function($ \sigma(z) = {1}/(1+e^{-z})$) as our activation function, we run into the issue of the output layer learning slowly. This happens because the valie of the $\sigma$ function becomes flat when it comes close to 0 or 1 and the the gradient of the activation function will tend to 0. The key takeaway from this is that the output layer of a will learn slowly if the output neuron is either low or high activation. We say here that the output neuron has saturated.</p>
<p>We can also extend this to previous layers using the second equation. If a particular neuron is close to saturation then the weights input of that neuron will also learn slowly. To overcome this we can design other activation
functions which have properties that are useful in our applications.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Activation-Functions">Activation Functions<a class="anchor-link" href="#Activation-Functions"> </a></h2><p>To overcome various issues with the Sigmoid activation function, other activation functions have been developed. I will go over some commonly used activation functions in this section:</p>
<h3 id="SoftMax">SoftMax<a class="anchor-link" href="#SoftMax"> </a></h3><p>The softmax function is commonly used in multi-calss classification as this provides a framework to calculate probabilities of each target calss over all possible target classes. It is defined as $$ softmax(x_i) = \frac{e^{x_i}}{\Sigma_j e^{x_j}}$$</p>
<p>The main advantage of this activation function is that the output ranges from 0 to 1 and the sum of probabilities will also sum up to 1. The implementation in numpy is quite straightforward as shown below:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span> 
<span class="kn">from</span> <span class="nn">tensorflow.nn</span> <span class="kn">import</span> <span class="n">softmax</span> <span class="k">as</span> <span class="n">tf_softmax</span>

<span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="nb">input</span><span class="p">)))</span>

<span class="nb">input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">tf_softmax</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">softmax</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tf.Tensor([0.38086243 0.17050068 0.44863688], shape=(3,), dtype=float64)
[0.38086243 0.17050068 0.44863688]
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="tanh">tanh<a class="anchor-link" href="#tanh"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-Backpropogartion-Algorithm">The Backpropogartion Algorithm<a class="anchor-link" href="#The-Backpropogartion-Algorithm"> </a></h2><p>The backpropagation equations provide us with a way of computing the gradient of the cost function. Let's explicitly write this out in the form of an algorithm:</p>
<ol>
<li>Input $x$: Set the corresponding activation $a_1$ for the input layer.</li>
<li>Feedforward: For each $l=1, 2 ,3 , â€¦ ,L$ compute $z^{l} = w^l a^{l-1}+b^l$ and $a^{l} = \sigma(z^{l})$</li>
<li>Output error $\delta_l$: Compute the vector $\delta^{L}
= \nabla_a C \odot \sigma'(z^L)$</li>
<li>Backpropagate the error: Compute the errors for each of the laters using $\delta^{l} = ((w^{l+1})^T \delta^{l+1}) \odot
\sigma'(z^{l})$</li>
<li>Output: The gradient of the cost function is given by $ \frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j$  and $ \frac{\partial C}{\partial b^l_j} = \delta^l_j $</li>
</ol>
<p>The backpropagation algorithm gives us a way of calculating the derivative of the cost function with respect ot each of the weights and biases in the network. Practically, this algorithm is used along with stochastic gradient descent while calculating the gradient for many training examples. We also split up the training examples into mini-batches and then apply a gradient descent learning step based on every mini batch.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span>
</pre></div>

    </div>
</div>
</div>

</div>
    

</div>



  </div><a class="u-url" href="/blog/2021/06/11/_06_02_Concept_Review.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>My Data Science Blog</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/nitinkash" title="nitinkash"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/1nitinkashyap" title="1nitinkashyap"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#linkedin"></use></svg></a></li><li><a rel="me" href="https://twitter.com/nikashyap" title="nikashyap"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
