{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings and their use in Search\n",
    "> \"Here I explore Embeddings in detail and see how they can be useful in Search and Retreival systems\"\n",
    "---\n",
    "- toc: false\n",
    "- layout: post\n",
    "- categories: [\"Neural NEtworks\", \"Deep Learning\", \"NLP\", \"WIP\"]\n",
    "- title: Embeddings in Search\n",
    "- hide: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0b8N2jfr5udV"
   },
   "source": [
    "## Embeddings:\n",
    "\n",
    "A new way for search using Neural Networks.\n",
    "\n",
    "\n",
    "Similar question retreival: Identify questions most closely related with the input question. This porblem deals with ranking the set of available questions with one specific questions using a similarity metric. Google has a huge database of questions and would like to solve the user's new question even if it is paraphrased differently. This is a problem of a much broader problem of search. When presented with a query, a search engine would like to search and find various entities like images, videos, documents, and even relavent ads. What is the best way to retreive these items?\n",
    "\n",
    "Retreival problems have been traditionally solved using an inverted-index since the 90's. An inverted index is basically a map between various words and where they are located. When the user queries a specific word, the system returns all the instances where that word was located. There are a bunch of subtle improvements like using Stemming or Regular Expressions to enhance the search functioanality but the overall idea has been to use an inverted-index to retrieve the candidates. The main issues with this method is that it is limited by word overlap, having a large number of search candidates containing the query makes it difficult for the user to find the exact result. Another issue is of finding other modalities like Images, Videos etc. This can be done by adding annotations etc but still the perfromance is still pretty bad as many items can be missed and annotations is a lengthy an d cumbersome process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QA3-LcDD_REJ"
   },
   "source": [
    "### Continuous Retreival:\n",
    "\n",
    "Here we are focusing on developing a model that ia able to learn to represent objects as vectors in a space that contains all objects. The model will also be able to learn to place similar objects close together (Eggs will be close to cooking eggs, pictures of eggs, of cooking but far away from something like dog or tiger). Another requirement of the model is for a way to be able to retrieve neighbouring objects fast so that the user may find the \"best page\" faster\n",
    "along with images, how-to videos and other relavent objects. These three cases are the most important requirements to develop a continuous retrieval system. \n",
    "\n",
    "Outline of the Objects:\n",
    "\n",
    "- Learn to represent objects and continuous vectors.\n",
    "- Learn to place similar objects close together\n",
    "- Learn to retrieve neighbouring object sfast.\n",
    "- Effects of neural retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ll1AQuUVPcbv"
   },
   "source": [
    "### Learn to represent objects and continuous vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sApoYnKVPkOW"
   },
   "source": [
    "Language Model: Can we predict the next word given a sequence of words. \n",
    "\n",
    "#### Using a one-hot Vector\n",
    "One way to solve this porblem is to use one-hot encoding to create a vector as long as the vocubulary and then each word will have a 1 for the index representing that particular word and have 0s in all the other positions. We would then use some classification clgortihm like Logistic Regression or Random FOrests to identify which word in the vocabulary would be the best for for the next word in the sequence. \n",
    "\n",
    "#### Using Embeddings:\n",
    "Another way to solve this porblem is not to use one hot encoding but to embed the words to an embedding space with arbitrary weights. We would then use back-propogartion to calculate these weights in the training phase where the model learns about various words and embeds each word in the new embedding space. The embedding space can be of varying dimensions depending on the size and nature of the problem at hand.\n",
    "\n",
    "Exploring these embeddings  will give us insights on how the model is learning and will also give us an opportunity to examine ans solve a lot more broader problems.\n",
    "\n",
    "> Word Embedding Space:\n",
    " We can see simiar words being places close to each other \n",
    "\n",
    " > Providing an opportunity to explore relationships. France -> Paris means Italy -> ____ . We can see if we can get Rome as a result by looking for results along the same vector line connecting France and Paris. This \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WY04JUcoPcoR"
   },
   "source": [
    "### Learn to place similar objects close together\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lcp5hHXaPkuQ"
   },
   "source": [
    "The history of NLP involves training models from small labelled datasets. One of Dual Encoder Model that helps encode The inputs into a fixed dimension representation of all the objects in that space. We acn pass images, qesutions etc through encoders which will translate the image, text, questions etc into a fixed dimensional space and using Cosine Similarity, we can identify if these objects are similar or not.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8MCmuREqlQ2g"
   },
   "source": [
    "#### Sampled Softmax\n",
    "\n",
    "We can use a sampled softymax to  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eAUoxIZCPdQN"
   },
   "source": [
    "### Learn to retrieve neighbouring objects quickly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MwKIj8ccPd2r"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DUEcncFBPn_G"
   },
   "source": [
    "### Effects of neural retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSDMEUq4PqB9"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AD0WacU45UPN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFAg-nrQ_RJX"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMfjXz0IzPeoakMO+8OtYY6",
   "collapsed_sections": [],
   "name": "2021-05-31-Embeddings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
