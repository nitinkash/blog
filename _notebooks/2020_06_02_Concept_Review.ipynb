{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/nitinkash/blog/blob/master/_notebooks/2020_06_02_Concept_Review.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Basic Concepts Overview\"\n",
    "> \"Review of basic concepts in Neural Networks and Deep Learning from this [site](http://neuralnetworksanddeeplearning.com/)\"\n",
    "\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [Neural Networks, Backpropagation, SGD, WIP]\n",
    "- hide: false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xh-WdqpA9q5P"
   },
   "source": [
    "While working on Kaggle Competetions, I realised that I had lost touch with the fundamentals of Neural Networks and though it was a good idea to go over these concepts again. While it is possible today to build models and make predictions without knowing all the details. Working with prediction models in a prefessional setting, I would personally like to know what I am doing so that I avoid potential pitfalls later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2a-XbMoq-jvO"
   },
   "source": [
    "# Backpropagation from scratch\n",
    "\n",
    "Backpropagation is a an algorithm introduced by [David Rumelhart](http://en.wikipedia.org/wiki/David_Rumelhart), [Geoffrey Hinton](http://www.cs.toronto.edu/~hinton/), and [Ronald Williams](http://en.wikipedia.org/wiki/Ronald_J._Williams) in this famous [paper](http://www.nature.com/nature/journal/v323/n6088/pdf/323533a0.pdf). The algorithm works much faster than earlier approaches to calculate the gradients of the cost function and essentailly making neural networks solve problems that were previously considered unsolvable. \n",
    "\n",
    "The algorithm calculates the partial derivative of the Cost function with respect to each and every weight and bias in the neural network. This is very  valuable as this gives us an intuitive and natural interpretation for each element in the network. I will now be going into the math behind the Backpropogation. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-inagfWpBk4"
   },
   "source": [
    "### Notation:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-WWuDb8RpMM8"
   },
   "source": [
    "## The four fundamental equations behind Backpropagation:\n",
    "\n",
    "Backpropgation is essentally a method to understand how changing the weights and biases in the neural network affects the cost function. Mathematically, we want to calculate $\\frac{\\partial C}{\\partial w_{jk}^{l}}$ and $\\frac{\\partial C}{\\partial b_{j}^{l}}$. Before we compute these we introduce an error term $ \\delta_{j}^{l}$ which represents the error in the $j^{th}$ neuron of the $l^{th}$ layer.\n",
    "\n",
    "### The First Equation\n",
    "The error term is calculated using the equation $$ \\delta_{j}^{l} = \\frac{\\partial C}{\\partial a_{j}^{L}} \\sigma'(z_{j}^{L})   $$\n",
    "\n",
    "This equation is a very intuitive result. The $\\partial C/ \\partial {a_{j}^{L}}$ term measures how fast the Cost function is changing with respect to the activations of the $j^{th}$ output activation. If the Cost Function does not depend on the output of a particular neuron $j$, then the term $ \\delta_{j}^{L} $ will be very small, which is as expected. The second term $ \\sigma'(z_{j}^{L}) $ measures how fast the activation function $ \\sigma$ is changing at $z_{j}^{L}$\n",
    "\n",
    "Everything in this equation of $\\delta _{j}^{L}$ can be computed fairly easily. The exact form of $\\partial C/ \\partial {a_{j}^{L}}$ will depend on the nature of the cost function used. If we use a quadratic cost function of the form $ C =\\frac{1}{2} \\Sigma_{j} (y_{j} - a_{j}^{L}) $ then the value of $\\partial C/ \\partial {a_{j}^{L}} = (a_{j}^{L} - y_{j})$ which can be computed easily. \n",
    "\n",
    "To avoid the index nightmare we can alse represent this as in matrix notation as $$ \\delta^L = \\nabla_{a} C \\odot \\sigma'(z^L) $$. Here, $ \\nabla_{a} C$ is basically a vector whose components are $\\partial C/ \\partial {a_{j}^{L}}$, the rate of change of $C$ with respect to the output activations. For a quadratic loss function $\\nabla_a C =(a^L-y)$ so the previous equation reduces to $$ \\delta^L = (a^L-y) \\odot \\sigma'(z^L). $$ \n",
    "\n",
    "Every component in this equation has a vector form that can be calculated easily using Numpy which is the goal of this exercise.\n",
    "\n",
    "### The Second Equation:\n",
    "\n",
    "The second equation of importance is used to calculate the error term $\\delta_l$ with the error in the next layer $\\delta_{l+1}$ \n",
    "\n",
    "$$\n",
    "\\delta^l = ((w^{l+1})^T \\delta^{l+1}) \\odot \\sigma'(z^l)\n",
    "$$\n",
    "\n",
    "where $w^{l+1} $ is the weight matrix for the $(l+1)^{th} $ layer. This equation seems complicated, but each element has a nice interpretation. Suppose we know the error $\\delta_{l+1}$ at the $l+1^{th}$ layer. When we apply the transpose weight matrix, $(w_{l+1})^T$, we can think intuitively of this as moving the error backward through the network. This gives us a measure of the error at the output of the $l^{th}$ layer. We can then take the Hadamard product $\\odot \\sigmaâ€²(z^l)$ to calculate the the error $\\delta_l$ in the weighted input to layer $l$.\n",
    "\n",
    "By combining the forst and second equations, we can compute the error $\\delta_l$ for any layer in the network. We start by using the first equation to compute $\\delta_L$ and then apply the second equation to compute $\\delta_{l-1}$, $\\delta_{l-2}$, and so on, all the way to the start of the network.\n",
    "\n",
    "\n",
    "### The Third equation:\n",
    "\n",
    "This equation is related to the rate of cahnge of the Cost function with respect to any bias term in the network. $$ \\frac{\\partial C}{\\partial b^l_j} =\n",
    "  \\delta^l_j $$\n",
    "\n",
    "This makes the calculation convenient as we already have methods to calculate the  $ \\delta_{j}^{l} $ using the first two equations. We can represent this as shorthand by removing the indices after understanding that  $\\delta$ is being evaluated at the same neuron as bias $b$.\n",
    "\n",
    "### The Fourth Equation:\n",
    "\n",
    "This equation fills in the missing piece by providing an expression to calculate the rate of change of the costfunction with respect to any weight in the network.\n",
    "\n",
    " $$ \\frac{\\partial C}{\\partial w^l_{jk}} = a^{l-1}_k \\delta^l_j $$\n",
    "\n",
    "This may seem complicated again but when we look at it in terms of a less index heavy representation we can see that it basically states that the rate of change of the Cost function because of the weights is the product of the activation of the neuron input with the error of the neuron output from the weight $w$. $$ \\frac{\\partial C}{\\partial w} = a_{\\rm in} \\delta_{\\rm out} $$\n",
    " \n",
    "\n",
    "A consequence of this equatioon is that the gradtent term $ \\partial C / \\partial w $ will be small if the activation $a_{in} \\approx 0$. This means that low-activation neurons will learn slowly as it does not change much during Gradient descent.\n",
    "\n",
    "Another insight from these equations is related to the nature of the activation function. If we use a Sigmoid function($ \\sigma(z) = {1}/(1+e^{-z})$) as our activation function, we run into the issue of the output layer learning slowly. This happens because the valie of the $\\sigma$ function becomes flat when it comes close to 0 or 1 and the the gradient of the activation function will tend to 0. The key takeaway from this is that the output layer of a will learn slowly if the output neuron is either low or high activation. We say here that the output neuron has saturated.\n",
    "\n",
    "We can also extend this to previous layers using the second equation. If a particular neuron is close to saturation then the weights input of that neuron will also learn slowly. To overcome this we can design other activation\n",
    "functions which have properties that are useful in our applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LcK9JlSJZFGw"
   },
   "source": [
    "## Activation Functions\n",
    "\n",
    "To overcome various issues with the Sigmoid activation function, other activation functions have been developed. I will go over some commonly used activation functions in this section:\n",
    "\n",
    "### SoftMax\n",
    "\n",
    "The softmax function is commonly used in multi-calss classification as this provides a framework to calculate probabilities of each target calss over all possible target classes. It is defined as $$ softmax(x_i) = \\frac{e^{x_i}}{\\Sigma_j e^{x_j}}$$ \n",
    "\n",
    "The main advantage of this activation function is that the output ranges from 0 to 1 and the sum of probabilities will also sum up to 1. The implementation in numpy is quite straightforward as shown below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "id": "uAeCb9zkbStC"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from tensorflow.nn import softmax as tf_softmax\n",
    "\n",
    "def softmax(input):\n",
    "    return np.exp(input) / float(sum(np.exp(input)))\n",
    "\n",
    "input = np.random.rand(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "id": "JAXaqcdDbpxj",
    "outputId": "ab93b4d7-1455-431b-b353-2f701ebb7266"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0.38086243 0.17050068 0.44863688], shape=(3,), dtype=float64)\n",
      "[0.38086243 0.17050068 0.44863688]\n"
     ]
    }
   ],
   "source": [
    "print(tf_softmax(input))\n",
    "print(softmax(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r7zKrN2ucXFZ"
   },
   "source": [
    "### tanh\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQwwQgHMwml7"
   },
   "source": [
    "## The Backpropogartion Algorithm\n",
    "\n",
    "The backpropagation equations provide us with a way of computing the gradient of the cost function. Let's explicitly write this out in the form of an algorithm:\n",
    "\n",
    "1. Input $x$: Set the corresponding activation $a_1$ for the input layer.\n",
    "2. Feedforward: For each $l=1, 2 ,3 , â€¦ ,L$ compute $z^{l} = w^l a^{l-1}+b^l$ and $a^{l} = \\sigma(z^{l})$\n",
    "3. Output error $\\delta_l$: Compute the vector $\\delta^{L}\n",
    "  = \\nabla_a C \\odot \\sigma'(z^L)$\n",
    "4. Backpropagate the error: Compute the errors for each of the laters using $\\delta^{l} = ((w^{l+1})^T \\delta^{l+1}) \\odot\n",
    "  \\sigma'(z^{l})$\n",
    "5. Output: The gradient of the cost function is given by $ \\frac{\\partial C}{\\partial w^l_{jk}} = a^{l-1}_k \\delta^l_j$  and $ \\frac{\\partial C}{\\partial b^l_j} = \\delta^l_j $\n",
    "\n",
    "The backpropagation algorithm gives us a way of calculating the derivative of the cost function with respect ot each of the weights and biases in the network. Practically, this algorithm is used along with stochastic gradient descent while calculating the gradient for many training examples. We also split up the training examples into mini-batches and then apply a gradient descent learning step based on every mini batch.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "id": "ZP0ai8K0-jde"
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "6gCXebTHM2Vt"
   },
   "outputs": [],
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOyIwAIkDh4r2czkOYEf0LB",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "2020-06-02-Concept-Review.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
