<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Embeddings in Search | Nitin Kashyap</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Embeddings in Search" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Here I explore Embeddings in detail and see how they can be useful in Search and Retreival systems" />
<meta property="og:description" content="Here I explore Embeddings in detail and see how they can be useful in Search and Retreival systems" />
<link rel="canonical" href="https://nitinkash.github.io/blog/neural%20networks/deep%20learning/nlp/wip/2021/05/31/Embeddings.html" />
<meta property="og:url" content="https://nitinkash.github.io/blog/neural%20networks/deep%20learning/nlp/wip/2021/05/31/Embeddings.html" />
<meta property="og:site_name" content="Nitin Kashyap" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-05-31T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"Here I explore Embeddings in detail and see how they can be useful in Search and Retreival systems","url":"https://nitinkash.github.io/blog/neural%20networks/deep%20learning/nlp/wip/2021/05/31/Embeddings.html","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://nitinkash.github.io/blog/neural%20networks/deep%20learning/nlp/wip/2021/05/31/Embeddings.html"},"headline":"Embeddings in Search","dateModified":"2021-05-31T00:00:00-05:00","datePublished":"2021-05-31T00:00:00-05:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://nitinkash.github.io/blog/feed.xml" title="Nitin Kashyap" /><link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" /><script src="https://hypothes.is/embed.js" async></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">Nitin Kashyap</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/">Home</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Embeddings in Search</h1><p class="page-description">Here I explore Embeddings in detail and see how they can be useful in Search and Retreival systems</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-05-31T00:00:00-05:00" itemprop="datePublished">
        May 31, 2021
      </time>
       â€¢ <span class="read-time" title="Estimated read time">
    
    
      4 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#Neural NEtworks">Neural NEtworks</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#Deep Learning">Deep Learning</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#NLP">NLP</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#WIP">WIP</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/nitinkash/blog/tree/master/_notebooks/2021-05-31-Embeddings.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/blog/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/nitinkash/blog/master?filepath=_notebooks%2F2021-05-31-Embeddings.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/blog/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/nitinkash/blog/blob/master/_notebooks/2021-05-31-Embeddings.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/blog/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-05-31-Embeddings.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Embeddings:">Embeddings:<a class="anchor-link" href="#Embeddings:"> </a></h2><p>A new way for search using Neural Networks.</p>
<p>Similar question retreival: Identify questions most closely related with the input question. This porblem deals with ranking the set of available questions with one specific questions using a similarity metric. Google has a huge database of questions and would like to solve the user's new question even if it is paraphrased differently. This is a problem of a much broader problem of search. When presented with a query, a search engine would like to search and find various entities like images, videos, documents, and even relavent ads. What is the best way to retreive these items?</p>
<p>Retreival problems have been traditionally solved using an inverted-index since the 90's. An inverted index is basically a map between various words and where they are located. When the user queries a specific word, the system returns all the instances where that word was located. There are a bunch of subtle improvements like using Stemming or Regular Expressions to enhance the search functioanality but the overall idea has been to use an inverted-index to retrieve the candidates. The main issues with this method is that it is limited by word overlap, having a large number of search candidates containing the query makes it difficult for the user to find the exact result. Another issue is of finding other modalities like Images, Videos etc. This can be done by adding annotations etc but still the perfromance is still pretty bad as many items can be missed and annotations is a lengthy an d cumbersome process.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Continuous-Retreival:">Continuous Retreival:<a class="anchor-link" href="#Continuous-Retreival:"> </a></h3><p>Here we are focusing on developing a model that ia able to learn to represent objects as vectors in a space that contains all objects. The model will also be able to learn to place similar objects close together (Eggs will be close to cooking eggs, pictures of eggs, of cooking but far away from something like dog or tiger). Another requirement of the model is for a way to be able to retrieve neighbouring objects fast so that the user may find the "best page" faster
along with images, how-to videos and other relavent objects. These three cases are the most important requirements to develop a continuous retrieval system.</p>
<p>Outline of the Objects:</p>
<ul>
<li>Learn to represent objects and continuous vectors.</li>
<li>Learn to place similar objects close together</li>
<li>Learn to retrieve neighbouring object sfast.</li>
<li>Effects of neural retrieval</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Learn-to-represent-objects-and-continuous-vectors.">Learn to represent objects and continuous vectors.<a class="anchor-link" href="#Learn-to-represent-objects-and-continuous-vectors."> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Language Model: Can we predict the next word given a sequence of words.</p>
<h4 id="Using-a-one-hot-Vector">Using a one-hot Vector<a class="anchor-link" href="#Using-a-one-hot-Vector"> </a></h4><p>One way to solve this porblem is to use one-hot encoding to create a vector as long as the vocubulary and then each word will have a 1 for the index representing that particular word and have 0s in all the other positions. We would then use some classification clgortihm like Logistic Regression or Random FOrests to identify which word in the vocabulary would be the best for for the next word in the sequence.</p>
<h4 id="Using-Embeddings:">Using Embeddings:<a class="anchor-link" href="#Using-Embeddings:"> </a></h4><p>Another way to solve this porblem is not to use one hot encoding but to embed the words to an embedding space with arbitrary weights. We would then use back-propogartion to calculate these weights in the training phase where the model learns about various words and embeds each word in the new embedding space. The embedding space can be of varying dimensions depending on the size and nature of the problem at hand.</p>
<p>Exploring these embeddings  will give us insights on how the model is learning and will also give us an opportunity to examine ans solve a lot more broader problems.</p>
<blockquote><p>Word Embedding Space:We can see simiar words being places close to each other 
Providing an opportunity to explore relationships. France -&gt; Paris means Italy -&gt; <em>__</em> . We can see if we can get Rome as a result by looking for results along the same vector line connecting France and Paris. This</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Learn-to-place-similar-objects-close-together">Learn to place similar objects close together<a class="anchor-link" href="#Learn-to-place-similar-objects-close-together"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The history of NLP involves training models from small labelled datasets. One of Dual Encoder Model that helps encode The inputs into a fixed dimension representation of all the objects in that space. We acn pass images, qesutions etc through encoders which will translate the image, text, questions etc into a fixed dimensional space and using Cosine Similarity, we can identify if these objects are similar or not.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Sampled-Softmax">Sampled Softmax<a class="anchor-link" href="#Sampled-Softmax"> </a></h4><p>We can use a sampled softymax to</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Learn-to-retrieve-neighbouring-objects-quickly">Learn to retrieve neighbouring objects quickly<a class="anchor-link" href="#Learn-to-retrieve-neighbouring-objects-quickly"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Effects-of-neural-retrieval">Effects of neural retrieval<a class="anchor-link" href="#Effects-of-neural-retrieval"> </a></h3>
</div>
</div>
</div>
</div>



  </div><a class="u-url" href="/blog/neural%20networks/deep%20learning/nlp/wip/2021/05/31/Embeddings.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>My Data Science Blog</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/nitinkash" title="nitinkash"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/1nitinkashyap" title="1nitinkashyap"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#linkedin"></use></svg></a></li><li><a rel="me" href="https://twitter.com/nikashyap" title="nikashyap"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
