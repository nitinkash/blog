<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Basic Concepts Overview | Nitin Kashyap</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Basic Concepts Overview" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Review of basic concepts in Neural Networks and Deep Learning" />
<meta property="og:description" content="Review of basic concepts in Neural Networks and Deep Learning" />
<link rel="canonical" href="https://nitinkash.github.io/blog/neural%20networks/backpropagation/sgd/wip/2021/06/22/_06_02_Concept_Review.html" />
<meta property="og:url" content="https://nitinkash.github.io/blog/neural%20networks/backpropagation/sgd/wip/2021/06/22/_06_02_Concept_Review.html" />
<meta property="og:site_name" content="Nitin Kashyap" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-06-22T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"Review of basic concepts in Neural Networks and Deep Learning","url":"https://nitinkash.github.io/blog/neural%20networks/backpropagation/sgd/wip/2021/06/22/_06_02_Concept_Review.html","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://nitinkash.github.io/blog/neural%20networks/backpropagation/sgd/wip/2021/06/22/_06_02_Concept_Review.html"},"headline":"Basic Concepts Overview","dateModified":"2021-06-22T00:00:00-05:00","datePublished":"2021-06-22T00:00:00-05:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://nitinkash.github.io/blog/feed.xml" title="Nitin Kashyap" /><link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" /><script src="https://hypothes.is/embed.js" async></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">Nitin Kashyap</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/">Home</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Basic Concepts Overview</h1><p class="page-description">Review of basic concepts in <a href='http://neuralnetworksanddeeplearning.com/'>Neural Networks and Deep Learning</a> </p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-06-22T00:00:00-05:00" itemprop="datePublished">
        Jun 22, 2021
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      8 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#Neural Networks">Neural Networks</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#Backpropagation">Backpropagation</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#SGD">SGD</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#WIP">WIP</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/nitinkash/blog/tree/master/_notebooks/2020_06_02_Concept_Review.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/blog/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/nitinkash/blog/master?filepath=_notebooks%2F2020_06_02_Concept_Review.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/blog/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/nitinkash/blog/blob/master/_notebooks/2020_06_02_Concept_Review.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/blog/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h1"><a href="#Backpropagation-from-scratch">Backpropagation from scratch </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Notation:">Notation: </a></li>
<li class="toc-entry toc-h2"><a href="#The-four-fundamental-equations-behind-Backpropagation:">The four fundamental equations behind Backpropagation: </a>
<ul>
<li class="toc-entry toc-h3"><a href="#The-First-Equation">The First Equation </a></li>
<li class="toc-entry toc-h3"><a href="#The-Second-Equation:">The Second Equation: </a></li>
<li class="toc-entry toc-h3"><a href="#The-Third-equation:">The Third equation: </a></li>
<li class="toc-entry toc-h3"><a href="#The-Fourth-Equation:">The Fourth Equation: </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#The-Backpropogartion-Algorithm">The Backpropogartion Algorithm </a></li>
<li class="toc-entry toc-h2"><a href="#Activation-Functions">Activation Functions </a>
<ul>
<li class="toc-entry toc-h3"><a href="#SoftMax">SoftMax </a></li>
<li class="toc-entry toc-h3"><a href="#tanh">tanh </a></li>
</ul>
</li>
</ul>
</li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020_06_02_Concept_Review.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="https://colab.research.google.com/github/nitinkash/blog/blob/master/_notebooks/2020_06_02_Concept_Review.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>While working on Kaggle Competetions, I realised that I had lost touch with the fundamentals of Neural Networks and though it was a good idea to go over these concepts again. While it is possible today to build models and make predictions without knowing all the details. Working with prediction models in a prefessional setting, I would personally like to know what I am doing so that I avoid potential pitfalls later.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Backpropagation-from-scratch">
<a class="anchor" href="#Backpropagation-from-scratch" aria-hidden="true"><span class="octicon octicon-link"></span></a>Backpropagation from scratch<a class="anchor-link" href="#Backpropagation-from-scratch"> </a>
</h1>
<p>Backpropagation is a an algorithm introduced by <a href="http://en.wikipedia.org/wiki/David_Rumelhart">David Rumelhart</a>, <a href="http://www.cs.toronto.edu/~hinton/">Geoffrey Hinton</a>, and <a href="http://en.wikipedia.org/wiki/Ronald_J._Williams">Ronald Williams</a> in this famous <a href="http://www.nature.com/nature/journal/v323/n6088/pdf/323533a0.pdf">paper</a>. The algorithm works much faster than earlier approaches to calculate the gradients of the cost function and essentailly making neural networks solve problems that were previously considered unsolvable.</p>
<p>The algorithm calculates the partial derivative of the Cost function with respect to each and every weight and bias in the neural network. This is very  valuable as this gives us an intuitive and natural interpretation for each element in the network. I will now be going into the math behind the Backpropogation.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Notation:">
<a class="anchor" href="#Notation:" aria-hidden="true"><span class="octicon octicon-link"></span></a>Notation:<a class="anchor-link" href="#Notation:"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-four-fundamental-equations-behind-Backpropagation:">
<a class="anchor" href="#The-four-fundamental-equations-behind-Backpropagation:" aria-hidden="true"><span class="octicon octicon-link"></span></a>The four fundamental equations behind Backpropagation:<a class="anchor-link" href="#The-four-fundamental-equations-behind-Backpropagation:"> </a>
</h2>
<p>Backpropgation is essentally a method to understand how changing the weights and biases in the neural network affects the cost function. Mathematically, we want to calculate $\frac{\partial C}{\partial w_{jk}^{l}}$ and $\frac{\partial C}{\partial b_{j}^{l}}$. Before we compute these we introduce an error term $ \delta_{j}^{l}$ which represents the error in the $j^{th}$ neuron of the $l^{th}$ layer.</p>
<h3 id="The-First-Equation">
<a class="anchor" href="#The-First-Equation" aria-hidden="true"><span class="octicon octicon-link"></span></a>The First Equation<a class="anchor-link" href="#The-First-Equation"> </a>
</h3>
<p>The error term is calculated using the equation $$ \delta_{j}^{l} = \frac{\partial C}{\partial a_{j}^{L}} \sigma'(z_{j}^{L})   $$</p>
<p>This equation is a very intuitive result. The $\partial C/ \partial {a_{j}^{L}}$ term measures how fast the Cost function is changing with respect to the activations of the $j^{th}$ output activation. If the Cost Function does not depend on the output of a particular neuron $j$, then the term $ \delta_{j}^{L} $ will be very small, which is as expected. The second term $ \sigma'(z_{j}^{L}) $ measures how fast the activation function $ \sigma$ is changing at $z_{j}^{L}$</p>
<p>Everything in this equation of $\delta _{j}^{L}$ can be computed fairly easily. The exact form of $\partial C/ \partial {a_{j}^{L}}$ will depend on the nature of the cost function used. If we use a quadratic cost function of the form $ C =\frac{1}{2} \Sigma_{j} (y_{j} - a_{j}^{L}) $ then the value of $\partial C/ \partial {a_{j}^{L}} = (a_{j}^{L} - y_{j})$ which can be computed easily.</p>
<p>To avoid the index nightmare we can represent this in matrix notation. To do so we need to use the Hadamard product which is represented using $ \odot$. This is basically an element wise multiplication operation between vectors of same dimensions. For example: 
$$\begin{eqnarray}
\left[\begin{array}{c} 1 \\ 3 \end{array}\right] 
  \odot \left[\begin{array}{c} 2 \\ 4\end{array} \right]
= \left[ \begin{array}{c} 1 * 2 \\ 3 * 4 \end{array} \right]
= \left[ \begin{array}{c} 2 \\ 12 \end{array} \right].
\tag{28}\end{eqnarray} $$</p>
<p>Using this we can arrive at this equation:</p>
$$ \delta^L = \nabla_{a} C \odot \sigma'(z^L) $$<p></p>
<p>Here, $ \nabla_{a} C$ is basically a vector whose components are $\partial C/ \partial {a_{j}^{L}}$, the rate of change of $C$ with respect to the output activations. For a quadratic loss function $\nabla_a C =(a^L-y)$ so the previous equation reduces to $$ \delta^L = (a^L-y) \odot \sigma'(z^L) $$</p>
<p>Now every component in the equation above has a vector form that can be calculated easily using Numpy which is the goal of this exercise.</p>
<h3 id="The-Second-Equation:">
<a class="anchor" href="#The-Second-Equation:" aria-hidden="true"><span class="octicon octicon-link"></span></a>The Second Equation:<a class="anchor-link" href="#The-Second-Equation:"> </a>
</h3>
<p>The second equation of importance is used to calculate the error term $\delta_l$ with the error in the next layer $\delta_{l+1}$</p>
$$
\delta^l = ((w^{l+1})^T \delta^{l+1}) \odot \sigma'(z^l)
$$<p>where $w^{l+1} $ is the weight matrix for the $(l+1)^{th} $ layer. This equation seems complicated, but each element has a nice interpretation. Suppose we know the error $\delta_{l+1}$ at the $l+1^{th}$ layer. When we apply the transpose weight matrix, $(w_{l+1})^T$, we can think intuitively of this as moving the error backward through the network. This gives us a measure of the error at the output of the $l^{th}$ layer. We can then take the Hadamard product $\odot \sigma′(z^l)$ to calculate the the error $\delta_l$ in the weighted input to layer $l$.</p>
<p>By combining the forst and second equations, we can compute the error $\delta_l$ for any layer in the network. We start by using the first equation to compute $\delta_L$ and then apply the second equation to compute $\delta_{l-1}$, $\delta_{l-2}$, and so on, all the way to the start of the network.</p>
<h3 id="The-Third-equation:">
<a class="anchor" href="#The-Third-equation:" aria-hidden="true"><span class="octicon octicon-link"></span></a>The Third equation:<a class="anchor-link" href="#The-Third-equation:"> </a>
</h3>
<p>This equation is related to the rate of cahnge of the Cost function with respect to any bias term in the network. $$ \frac{\partial C}{\partial b^l_j} =
  \delta^l_j $$</p>
<p>This makes the calculation convenient as we already have methods to calculate the  $ \delta_{j}^{l} $ using the first two equations. We can represent this as shorthand by removing the indices after understanding that  $\delta$ is being evaluated at the same neuron as bias $b$.</p>
<h3 id="The-Fourth-Equation:">
<a class="anchor" href="#The-Fourth-Equation:" aria-hidden="true"><span class="octicon octicon-link"></span></a>The Fourth Equation:<a class="anchor-link" href="#The-Fourth-Equation:"> </a>
</h3>
<p>This equation fills in the missing piece by providing an expression to calculate the rate of change of the costfunction with respect to any weight in the network.</p>
<p>$$ \frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j $$</p>
<p>This may seem complicated again but when we look at it in terms of a less index heavy representation we can see that it basically states that the rate of change of the Cost function because of the weights is the product of the activation of the neuron input with the error of the neuron output from the weight $w$. $$ \frac{\partial C}{\partial w} = a_{\rm in} \delta_{\rm out} $$</p>
<p>A consequence of this equatioon is that the gradtent term $ \partial C / \partial w $ will be small if the activation $a_{in} \approx 0$. This means that low-activation neurons will learn slowly as it does not change much during Gradient descent.</p>
<p>Another insight from these equations is related to the nature of the activation function. If we use a Sigmoid function($ \sigma(z) = {1}/(1+e^{-z})$) as our activation function, we run into the issue of the output layer learning slowly. This happens because the valie of the $\sigma$ function becomes flat when it comes close to 0 or 1 and the the gradient of the activation function will tend to 0. The key takeaway from this is that the output layer of a will learn slowly if the output neuron is either low or high activation. We say here that the output neuron has saturated.</p>
<p>We can also extend this to previous layers using the second equation. If a particular neuron is close to saturation then the weights input of that neuron will also learn slowly. To overcome this we can design other activation functions which have properties that are useful in our applications.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-Backpropogartion-Algorithm">
<a class="anchor" href="#The-Backpropogartion-Algorithm" aria-hidden="true"><span class="octicon octicon-link"></span></a>The Backpropogartion Algorithm<a class="anchor-link" href="#The-Backpropogartion-Algorithm"> </a>
</h2>
<p>The backpropagation equations provide us with a way of computing the gradient of the cost function. Let's explicitly write this out in the form of an algorithm:</p>
<ol>
<li>Input $x$: Set the corresponding activation $a_1$ for the input layer.</li>
<li>Feedforward: For each $l=1, 2 ,3 , … ,L$ compute $z^{l} = w^l a^{l-1}+b^l$ and $a^{l} = \sigma(z^{l})$</li>
<li>Output error $\delta_l$: Compute the vector $\delta^{L}
= \nabla_a C \odot \sigma'(z^L)$</li>
<li>Backpropagate the error: Compute the errors for each of the laters using $\delta^{l} = ((w^{l+1})^T \delta^{l+1}) \odot
\sigma'(z^{l})$</li>
<li>Output: The gradient of the cost function is given by $ \frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j$  and $ \frac{\partial C}{\partial b^l_j} = \delta^l_j $</li>
</ol>
<p>The backpropagation algorithm gives us a way of calculating the derivative of the cost function with respect ot each of the weights and biases in the network. Practically, this algorithm is used along with stochastic gradient descent while calculating the gradient for many training examples. We also split up the training examples into mini-batches and then apply a gradient descent learning step based on every mini batch.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Activation-Functions">
<a class="anchor" href="#Activation-Functions" aria-hidden="true"><span class="octicon octicon-link"></span></a>Activation Functions<a class="anchor-link" href="#Activation-Functions"> </a>
</h2>
<p>From the previous section, we saw that there is actually no real reason to restrict the choice of the activation function to just the sigmoid. Based on the task at hand we can adjust and use other activation functions based on the desired properties. I will explore some activation functions in this section:</p>
<h3 id="SoftMax">
<a class="anchor" href="#SoftMax" aria-hidden="true"><span class="octicon octicon-link"></span></a>SoftMax<a class="anchor-link" href="#SoftMax"> </a>
</h3>
<p>The softmax function is commonly used in multi-calss classification as this provides a framework to calculate probabilities of each target calss over all possible target classes. It is defined as $$ softmax(x_i) = \frac{e^{x_i}}{\Sigma_j e^{x_j}}$$</p>
<p>The main advantage of this activation function is that the output ranges from 0 to 1 and the sum of probabilities will also sum up to 1. The implementation in numpy is quite straightforward as shown below:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span> 
<span class="kn">from</span> <span class="nn">tensorflow.nn</span> <span class="kn">import</span> <span class="n">softmax</span> <span class="k">as</span> <span class="n">tf_softmax</span>

<span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="nb">input</span><span class="p">)))</span>

<span class="nb">input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">tf_softmax</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">softmax</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tf.Tensor([0.38086243 0.17050068 0.44863688], shape=(3,), dtype=float64)
[0.38086243 0.17050068 0.44863688]
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="tanh">
<a class="anchor" href="#tanh" aria-hidden="true"><span class="octicon octicon-link"></span></a>tanh<a class="anchor-link" href="#tanh"> </a>
</h3>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span>
</pre></div>

    </div>
</div>
</div>

</div>
    

</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="nitinkash/blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/blog/neural%20networks/backpropagation/sgd/wip/2021/06/22/_06_02_Concept_Review.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>My Data Science Blog</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/nitinkash" title="nitinkash"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/1nitinkashyap" title="1nitinkashyap"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#linkedin"></use></svg></a></li><li><a rel="me" href="https://twitter.com/nikashyap" title="nikashyap"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
