{
  
    
        "post0": {
            "title": "Backpropagation from scratch",
            "content": ". While going through and trying to make a text generator using GANs I realised that my fundamentals in Neural Networks is quite weak an&gt; it had been nearly 4 yoers since I finished my Masters Degree and I had lost touch with a lot of key concepts of Neural NEtworks. While it is possible to do most of these things without knowing all the details. I would personally like to know what I am using so that I avoid potential pitfalls later. I also came across this post while reviewing the videos in Stanford&#39;s CS 211n Course. link. So will be dopign some porblems from the course assignment and reviewing some key concepts here. . Notation: . The four fundamental equations behind Backpropagation: . Backpropgation is essentally a method to understand how changing the weights and biases in the neural network affects the cost function. Mathematically, we want to calculate $ frac{ partial C}{ partial w_{jk}^{l}}$ and $ frac{ partial C}{ partial b_{j}^{l}}$. Before we compute these we introduce an error term $ delta_{j}^{l}$ which represents the error in the $j^{th}$ neuron of the $l^{th}$ layer. . The First Equation . The error term is calculated using the equation $$ delta_{j}^{l} = frac{ partial C}{ partial a_{j}^{L}} sigma&#39;(z_{j}^{L}) $$ . This equation is a very intuitive result. The $ partial C/ partial {a_{j}^{L}}$ term measures how fast the Cost function is changing with respect to the activations of the $j^{th}$ output activation. If the Cost Function does not depend on the output of a particular neuron $j$, then the term $ delta_{j}^{L} $ will be very small, which is as expected. The second term $ sigma&#39;(z_{j}^{L}) $ measures how fast the activation function $ sigma$ is changing at $z_{j}^{L}$ . Everything in this equation of $ delta _{j}^{L}$ can be computed fairly easily. The exact form of $ partial C/ partial {a_{j}^{L}}$ will depend on the nature of the cost function used. If we use a quadratic cost function of the form $ C = frac{1}{2} Sigma_{j} (y_{j} - a_{j}^{L}) $ then the value of $ partial C/ partial {a_{j}^{L}} = (a_{j}^{L} - y_{j})$ which can be computed easily. . To avoid the index nightmare we can alse represent this as in matrix notation as $$ delta^L = nabla_{a} C odot sigma&#39;(z^L) $$. Here, $ nabla_{a} C$ is basically a vector whose components are $ partial C/ partial {a_{j}^{L}}$, the rate of change of $C$ with respect to the output activations. For a quadratic loss function $ nabla_a C =(a^L-y)$ so the previous equation reduces to $$ delta^L = (a^L-y) odot sigma&#39;(z^L). $$ . Every component in this equation has a vector form that can be calculated easily using Numpy which is the goal of this exercise. . The Second Equation: . The second equation of importance is used to calculate the error term $ delta_l$ with the error in the next layer $ delta_{l+1}$ . $$ delta^l = ((w^{l+1})^T delta^{l+1}) odot sigma&#39;(z^l) $$where $w^{l+1} $ is the weight matrix for the $(l+1)^{th} $ layer. This equation seems complicated, but each element has a nice interpretation. Suppose we know the error $ delta_{l+1}$ at the $l+1^{th}$ layer. When we apply the transpose weight matrix, $(w_{l+1})^T$, we can think intuitively of this as moving the error backward through the network. This gives us a measure of the error at the output of the $l^{th}$ layer. We can then take the Hadamard product $ odot sigma′(z^l)$ to calculate the the error $ delta_l$ in the weighted input to layer $l$. . By combining the forst and second equations, we can compute the error $ delta_l$ for any layer in the network. We start by using the first equation to compute $ delta_L$ and then apply the second equation to compute $ delta_{l-1}$, $ delta_{l-2}$, and so on, all the way to the start of the network. . The Third equation: . This equation is related to the rate of cahnge of the Cost function with respect to any bias term in the network. $$ frac{ partial C}{ partial b^l_j} = delta^l_j $$ . This makes the calculation convenient as we already have methods to calculate the $ delta_{j}^{l} $ using the first two equations. We can represent this as shorthand by removing the indices after understanding that $ delta$ is being evaluated at the same neuron as bias $b$. . The Fourth Equation: . This equation fills in the missing piece by providing an expression to calculate the rate of change of the costfunction with respect to any weight in the network. . $$ frac{ partial C}{ partial w^l_{jk}} = a^{l-1}_k delta^l_j $$ . This may seem complicated again but when we look at it in terms of a less index heavy representation we can see that it basically states that the rate of change of the Cost function because of the weights is the product of the activation of the neuron input with the error of the neuron output from the weight $w$. $$ frac{ partial C}{ partial w} = a_{ rm in} delta_{ rm out} $$ . A consequence of this equatioon is that the gradtent term $ partial C / partial w $ will be small if the activation $a_{in} approx 0$. This means that low-activation neurons will learn slowly as it does not change much during Gradient descent. . Another insight from these equations is related to the nature of the activation function. If we use a Sigmoid function($ sigma(z) = {1}/(1+e^{-z})$) as our activation function, we run into the issue of the output layer learning slowly. This happens because the valie of the $ sigma$ function becomes flat when it comes close to 0 or 1 and the the gradient of the activation function will tend to 0. The key takeaway from this is that the output layer of a will learn slowly if the output neuron is either low or high activation. We say here that the output neuron has saturated. . We can also extend this to previous layers using the second equation. If a particular neuron is close to saturation then the weights input of that neuron will also learn slowly. To overcome this we can design other activation functions which have properties that are useful in our applications. . Activation Functions . To overcome various issues with the Sigmoid activation function, other activation functions have been developed. I will go over some commonly used activation functions in this section: . SoftMax . The softmax function is commonly used in multi-calss classification as this provides a framework to calculate probabilities of each target calss over all possible target classes. It is defined as $$ softmax(x_i) = frac{e^{x_i}}{ Sigma_j e^{x_j}}$$ . The main advantage of this activation function is that the output ranges from 0 to 1 and the sum of probabilities will also sum up to 1. The implementation in numpy is quite straightforward as shown below: . import numpy as np from tensorflow.nn import softmax as tf_softmax def softmax(input): return np.exp(input) / float(sum(np.exp(input))) input = np.random.rand(3) . print(tf_softmax(input)) print(softmax(input)) . tf.Tensor([0.38086243 0.17050068 0.44863688], shape=(3,), dtype=float64) [0.38086243 0.17050068 0.44863688] . tanh . The Backpropogartion Algorithm . The backpropagation equations provide us with a way of computing the gradient of the cost function. Let&#39;s explicitly write this out in the form of an algorithm: . Input $x$: Set the corresponding activation $a_1$ for the input layer. | Feedforward: For each $l=1, 2 ,3 , … ,L$ compute $z^{l} = w^l a^{l-1}+b^l$ and $a^{l} = sigma(z^{l})$ | Output error $ delta_l$: Compute the vector $ delta^{L} = nabla_a C odot sigma&#39;(z^L)$ | Backpropagate the error: Compute the errors for each of the laters using $ delta^{l} = ((w^{l+1})^T delta^{l+1}) odot sigma&#39;(z^{l})$ | Output: The gradient of the cost function is given by $ frac{ partial C}{ partial w^l_{jk}} = a^{l-1}_k delta^l_j$ and $ frac{ partial C}{ partial b^l_j} = delta^l_j $ | The backpropagation algorithm gives us a way of calculating the derivative of the cost function with respect ot each of the weights and biases in the network. Practically, this algorithm is used along with stochastic gradient descent while calculating the gradient for many training examples. We also split up the training examples into mini-batches and then apply a gradient descent learning step based on every mini batch. . import torch . .",
            "url": "https://nitinkash.github.io/blog/2021/06/11/_06_02_Concept_Review.html",
            "relUrl": "/2021/06/11/_06_02_Concept_Review.html",
            "date": " • Jun 11, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Recurrent Neural Networks",
            "content": "A recurrent neural network (RNN) is a type of Neural Network that allows previous outputs to be used as inputs while using hidden states. It is called a Recurrent Network because it repeatedly takes an input, uses it to modify a hidden layer and then provides an output which is then fed back to the hidden layer along with the next input. The hidden layer acts as “memory” that keeps track of the previous inputs by how the weights in the layer were modified by the said input. Consider an example of a RNN working on predicting the next word in a sentence. The first word is passed into a hidden layer, the output of this hidden layer along with the next input is then passed on to the next hidden layer and this process continues till all the inputs are passed on to the final output layer which predicts the final word. In this way, the RNN has an “understanding” of each word in the network and can make a reasonable prediction of the final word. . TODO: Add image of RNN for predicting the next word here:: . The structure used here is a many-to-one type of RNN. Many other RNN architectures have been developed to accomplish other tasks like music generation, sentiment analysis and machine translation. .",
            "url": "https://nitinkash.github.io/blog/deep%20learning/nlp/2021/05/19/RNN-Overview.html",
            "relUrl": "/deep%20learning/nlp/2021/05/19/RNN-Overview.html",
            "date": " • May 19, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Q01 Introduction",
            "content": "Introduction . Do you need these for deep learning? . Lots of math T / F: False, a lot of libraries have been created to take care of the complex math parts and allow us to focus on training models | Lots of data T / F: False, a lot of great results have been seen with small amounts of data | Lots of expensive computers T / F: False, there are a lot of free and cheap options available to create and deploy deep learning models | A PhD T / F: False | . | Name five areas where deep learning is now the best in the world. Image Recognition | Natural Language Processing | Recommendation Systems | Playing games like chess, Go etc. | Image generation | . | What was the name of the first device that was based on the principle of the artificial neuron? The Perceptron | . | Based on the book of the same name, what are the requirements for parallel distributed processing (PDP)? Processing Units | State of Activation | Output Function | Pattern of connectivity | Propagation Rule | Activation Rule | Learning rule | Environment. | . | What were the two theoretical misunderstandings that held back the field of neural networks? Cannot learn simple functions like XOR | Require a lot of computational power and was very slow. | . | What is a GPU? Graphical Processing Unit. Specialized CPU working at a faster speed. | . | Why is it hard to use a traditional computer program to recognize images in a photo? It is very complex to create a logic for reading in the pixels and performing a method of recognition, because we ourselves can’t write these steps down. These happen automatically in our brains without us being conscious about it. | . | What did Samuel mean by “weight assignment”? By weight assignment, Arthur Samuel means to assign a particular set of values to the different variables(weights) involved in the program. The inputs and weights are the two different values involved in a program. | . | What term do we normally use in deep learning for what Samuel called “weights”? They are also called parameters | . | Draw a picture that summarizes Samuel’s view of a machine learning model. | . | Why is it hard to understand why a deep learning model makes a particular prediction? The Deep learning models usually sift through enormous amounts of data to find data and patterns that may not be noticed by human experts. These patterns are usually difficult to isolate and make sense of in isolation and might by confusing to interpret. | . | What is the name of the theorem that shows that a neural network can solve any mathematical problem to any level of accuracy? Universal approximation theorem | . | What do you need in order to train a model? Data, a set of weights and | . | How could a feedback loop impact the rollout of a predictive policing model? This model will predict where more arrests will be made. If there is a bias in the data collection, the model will predict those areas where previous arrests had been high. After seeing these results, the police departments may assign more policemen to patrol areas which have a high probability of arrests which may cause even more arrests being made. | . | What is the difference between classification and regression? Classification involves predicting discrete classes which the elements belong into and regression involves predicting a continuous variable for a given set of independent variables. | . | What is a validation set? What is a test set? Why do we need them? While training a deep learning model, we usually train the model on a fraction of the training data and do not show the rest of the data to the model while training. The segment of data not shown to the model during training is called a validation set. It is used to measure the accuracy of the model. Another portion of the data is used to evaluate the performance of the model, this data is kept away from the model until all the hyper-parameters are tuned and modelling phases are completed. The evaluation on the final test set gives us a final assessment of our model. | . | What will fastai do if you don’t provide a validation set? Automatically create a validation set with 20% of the training data. | . | Can we always use a random sample for a validation set? Why or why not? No. Sometimes if we are dealing with time series data, using random samples for a validation set will cause misleading results. | . | What is overfitting? Provide an example. Training a model so that it remembers specific features of the input data, rather than generalizing well to data not seen during training | . | What is a metric? How does it differ from “loss”? A metric is a measure by which we can assess hot the model is performing on the validation set. Loss is a measure of how good the model is. The metric is used for the users to understand the model while the loss is what the computer uses during the training phase to define what steps need to be taken. | . | How can pretrained models help? Pretrained models can help in reducing the time taken for training and | . | What is the “head” of a model? That specific part of the model added to be specific to a particular dataset is called the head of the model | . | What kinds of features do the early layers of a CNN find? How about the later layers? The early layers of the CNN identified vertical, diagonal and horizontal lines. Later layers identified repeating lines, circles and increasingly complex patterns. | . | Are image models only useful for photos? No, sounds have been transformed to images based on their frequencies/loudness and then passed on to image models to identify the type of sound. | . | What is an “architecture”? Architecture refers to the template of the model that we’re trying to fit; i.e., the actual mathematical function that we’re passing the input data and parameters to | . | What is segmentation? Creating a model that can identify distinct objects (vehicles, pedestrians etc.) in a picture is called segmentation. | . | What is y_range used for? When do we need it? y_range is a parameter that defines the range of values the prediction(target) can take. | . | What are “hyperparameters”? Hyperparameters are basically parameters about parameters. They affect how fast and accurate the deep learning model performs | . | What’s the best way to avoid failures when using AI in an organization? Make sure the model is generalized, scalable and works on data not used to train the model. Another useful method is to use a simple model and compare the performance of the “expert” model with the simple baseline to get a sense of the accuracy of the model. | . | .",
            "url": "https://nitinkash.github.io/blog/markdown/questionnaire/2021/04/20/Jargon-Review.html",
            "relUrl": "/markdown/questionnaire/2021/04/20/Jargon-Review.html",
            "date": " • Apr 20, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Q02 Production",
            "content": "Production: . Provide an example of where the bear classification model might work poorly in production, due to structural or style differences in the training data. | Where do text models currently have a major deficiency? | What are possible negative societal implications of text generation models? | In situations where a model might make mistakes, and those mistakes could be harmful, what is a good alternative to automating a process? | What kind of tabular data is deep learning particularly good at? | What’s a key downside of directly using a deep learning model for recommendation systems? | What are the steps of the Drivetrain Approach? | How do the steps of the Drivetrain Approach map to a recommendation system? | Create an image recognition model using data you curate, and deploy it on the web. | What is DataLoaders? | What four things do we need to tell fastai to create DataLoaders? | What does the splitter parameter to DataBlock do? | How do we ensure a random split always gives the same validation set? | What letters are often used to signify the independent and dependent variables? | What’s the difference between the crop, pad, and squish resize approaches? When might you choose one over the others? | What is data augmentation? Why is it needed? | What is the difference between item_tfms and batch_tfms? | What is a confusion matrix? | What does export save? | What is it called when we use a model for getting predictions, instead of training? | What are IPython widgets? | When might you want to use CPU for deployment? When might GPU be better? | What are the downsides of deploying your app to a server, instead of to a client (or edge) device such as a phone or PC? | What are three examples of problems that could occur when rolling out a bear warning system in practice? | What is “out-of-domain data”? | What is “domain shift”? | What are the three steps in the deployment process? | .",
            "url": "https://nitinkash.github.io/blog/markdown/questionnaire/2021/04/20/Jargon-Review-2.html",
            "relUrl": "/markdown/questionnaire/2021/04/20/Jargon-Review-2.html",
            "date": " • Apr 20, 2021"
        }
        
    
  
    
  
    
        ,"post5": {
            "title": "Why?",
            "content": "Why? Why start a blog now? . Right now it’s May 2021, I am in India while the whole country is battling the second wave of COVID-19 infections. Every day gets more depressing than the last, bringing news of new cases and deaths to someone I know. It’s a really depressing environment and being locked up at home makes it so much worse. Watching COVID-19 rip up families and destroy the livelihood of nearly every other person was incredibly painful to watch. As news unfolded of even crematoriums being so over-worked that their steel chimneys were started to melt I got a sense of how bad the damage was. . With death being such a real possibility, I started to think about how I had lived my life and if it was my last day was to come in the near future how would I live these potential last few days. Would I want to spend them sitting around at home moping on about how helpless I felt? This was pretty much the same question that Steve Jobs contemplated upon and referred to in his Stanford commencement address. After rewatching that for the umpteenth time I decided to think about how I was spending my life and what regrets I would have if today was my last day. . My top regrets: . So the first thing that came to my mind was that I was not going to be able to visit all the places I wanted to. I still wanted to see, experience and accomplish so many things like going on a summit expedition, climbing my To-Do routes, Freediving, seeing Mt Everest, etc. Thinking of these made me feel privileged and the virtue-signalling part of me told me to stop. Also, these were impossible to do right now from home and all I could do to be in the best shape to do this was to stay healthy and workout from home. | So I narrowed my options into what would I regret not doing if I was indoors. If I was forced to stay locked down at home and if today was my last day, what would I do? The first thing that came to mind was to finish the fastai course I had started a few weeks back. I was looking to learn about building Neural Networks and Deep Learning and try to build applications that would supplement my life as a derivatives trader. If I were to die soon, then I would probably do what I love, which was to build cool things with computers. In some crazy way this was a great time to learn a new skill. | . Which brings me to this blog. How do I teach myself something like Deep Learning where there is no one to teach and give me feedback. I came across a solution from this article about the Feynman Method. Basically, with this blog I intend to explain difficult concepts to myself and to also stay motivated during the course. You will probably find the things explained here, explained better elsewhere. For now, I am going to focus on the following. . the Deep learning course of fast.ai | . I will update this list the longer I stay alive. Let’s stop moping and get started .",
            "url": "https://nitinkash.github.io/blog/general/2020/12/20/HowtoLearn.html",
            "relUrl": "/general/2020/12/20/HowtoLearn.html",
            "date": " • Dec 20, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://nitinkash.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "My name is Nitin Kashyap and I am primarily a consultant who helps people and companies solve problems using data science and analytics. These could include anything from automating tasks and wrangling data, creating forecasting models to improve decision making and helping identify issues to help improve customer satisfaction. I have experience in creating forecasting models, creating automated scripts for data collection, web scraping, and Social Media automation. . I have a wide areas of interests from photography and rock climbing to Social Media analytics and Application Development to Derivatives Trading. Currently I am learning Deep Learning and hope to ultimately build models to help me Day trade in the stock market. Most of my blog posts on here describe my journey in learning Deep Learning. I intend to create some cool applications of Deep Learning and add them here as I move on. . I am originally from Bangalore, India and I have a background in Operations Research and Optimization. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://nitinkash.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://nitinkash.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}